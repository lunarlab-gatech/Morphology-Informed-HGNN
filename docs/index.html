<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="MI-HGNN is a structured deep learning model that incorporates robot morphological constraints; designed as a learning-based contact estimator for legged robots.">
  <meta property="og:title" content="MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception"/>
  <meta property="og:description" content="MI-HGNN is a structured deep learning model that incorporates robot morphological constraints; designed as a learning-based contact estimator for legged robots."/>
  <meta property="og:url" content="https://lunarlab-gatech.github.io/Morphology-Informed-HGNN/"/>

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="https://raw.githubusercontent.com/lunarlab-gatech/Morphology-Informed-HGNN/refs/heads/main/paper/website_images/banner_image.png"/>
  <meta property="og:type" content="article">
  <meta property="og:locale" content="en_US">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="https://raw.githubusercontent.com/lunarlab-gatech/Morphology-Informed-HGNN/refs/heads/main/paper/website_images/banner_image.png">
  <meta name="twitter:card" content="MI-HGNN is a structured deep learning model that incorporates robot morphological constraints; designed as a learning-based contact estimator for legged robots.">
  <meta name="twitter:title" content="MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception">
  <meta name="twitter:description" content="MI-HGNN is a structured deep learning model that incorporates robot morphological constraints; designed as a learning-based contact estimator for legged robots.">

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="robotics, deep learning, structured deep learning, contact perception, morphology, graph neural network, legged robots, quadrupeds, robot perception, perception, heterogeneous graph neural network, MI-HGNN, mi-hgnn, gnn, hgnn">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception</title>
  <link rel="icon" type="image/x-icon" href="https://raw.githubusercontent.com/lunarlab-gatech/Morphology-Informed-HGNN/refs/heads/main/paper/website_images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=i5PF63IAAAAJ&hl=en" target="_blank">Daniel Butterfield</a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=en&user=Sdbn4VYAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Sandilya Sai Garimella</a>,</span>
                  <span class="author-block">
                    <a href="https://sciprofiles.com/profile/3891456" target="_blank">Nai-Jen Cheng</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://ganlumomo.github.io/" target="_blank">Lu Gan</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Georgia Institute of Technology<br>Accepted to ICRA 2025</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2409.11146" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!--<span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/lunarlab-gatech/Morphology-Informed-HGNN" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2409.11146" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body is-centered"> -->
      <!-- Your image here -->
      <!-- <img src="https://raw.githubusercontent.com/lunarlab-gatech/Morphology-Informed-HGNN/refs/heads/main/paper/website_images/figure1.png" alt="Figure 1; Visualization of our MI-HGNN for the Mini-Cheetah robot as an example." width="600" class="center"/> -->
      <!-- <h2 class="subtitle has-text-centered top-margin">
        Visualization of our MI-HGNN for the Mini-Cheetah robot as an example. The structure and connectivity of our graph is constructed from the robot morphology. Local sensor measurements are embedded into the corresponding node to predict contact information at the foot node.
      </h2> -->
    <!-- </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section center hero is-light">
<div class="container-side-by-side-abstract">
  <div class="container" style="display: flex; justify-content: center; min-width: 0px;">
    <div class="container" style="flex-grow: 3;">
    </div>
    <div class="container" style="display: flex; justify-content: center; min-width: 0px; margin-bottom: 2%;">
      <!-- Your image here -->
      <img src="https://raw.githubusercontent.com/lunarlab-gatech/Morphology-Informed-HGNN/refs/heads/main/paper/website_images/figure1.png" alt="Figure 1; Visualization of our MI-HGNN for the Mini-Cheetah robot as an example." style="max-width: 550px; margin-left: 3%; margin-right: 8%; min-width: 0px;"/>
    </div>
  </div>
  <div class="container has-text-justified center" style="display:flex; justify-content: flex-start;">
    <div class="container has-text-justified center" style="display:flex; flex-direction: column; max-width: 600px; flex-shrink: 1; justify-content: flex-start;">
      <h2 class="title is-3 center">Abstract</h2>
      <p style="overflow: auto;">
        We present a Morphology-Informed Heterogeneous Graph Neural Network (MI-HGNN) for learning-based contact perception. The architecture and connectivity of the MI-HGNN are constructed from the robot morphology, in which nodes and edges are robot joints and links, respectively. By incorporating the morphology-informed constraints into a neural network, we improve a learning-based approach using model-based knowledge. We apply the proposed MI-HGNN to two contact perception problems, and conduct extensive experiments using both real-world and simulated data collected using two quadruped robots. Our experiments demonstrate the superiority of our method in terms of effectiveness, generalization ability, model efficiency, and sample efficiency. Our MI-HGNN improved the performance of a state-of-the-art model that leverages robot morphological symmetry by 8.4% with only 0.21% of its parameters. Although MI-HGNN is applied to contact perception problems for legged robots in this work, it can be seamlessly applied to other types of multi-body dynamical systems and has the potential to improve other robot learning frameworks. Our code is made publicly available at <a href="https://github.com/lunarlab-gatech/Morphology-Informed-HGNN" target="_blank">https://github.com/lunarlab-gatech/Morphology-Informed-HGNN</a>.
      </p>
    </div>
    <div class="container" style="flex-grow: 3;"></div>
  </div>
</div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item">
        <!-- Your image here -->
        <h1 class="title">How does MI-HGNN work?</h1>
        <br>
        <img src="https://raw.githubusercontent.com/lunarlab-gatech/Morphology-Informed-HGNN/refs/heads/main/paper/website_images/figure2.png" alt="Figure 2; Overview of the proposed MI-HGNN for legged robot contact perception problems."  class="center" width="1200"/>
        <br>
        <p class="subtitle top-margin">
          Our MI-HGNN is constructed from a robot kinematic structure where nodes are joints and edges are links. Proprioceptive sensor measurements acquired at each local frame are embedded into the corresponding node through a heterogeneous encoder, and fused via several layers of message-passing. A foot decoder attached to the foot node exacts the contact information during inference.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item">
        <h1 class="title">Why does MI-HGNN work?</h1>
        <br>
         <p class="subtitle">
          <strong>A data correlation aspect:</strong> MI-HGNN constrains the learning problem based on the robot's morphology and explicitly captures the correlation and causality between inputs. Message passing between nodes is influenced by intermediate nodes, resembling the physical laws of the robot's system, thereby embedding causality into the message-passing process.
          <br><br>
          <strong>A symmetry aspect:</strong> Learned weights are identical between all morphologically-identical limbs of the robotic platform. In addition to reducing the complexity of the search space and lowering model sizes, it allows the model to quickly learn a more generalizable function when compared to completely unconstrained learning models.
         </p>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item">
        <h1 class="title">
          What are the benefits of MI-HGNN?
        </h1>
      </div>
      
      <div class="container-side-by-side">
        <div class="text" style="margin-right: 75px;">
          <p class="subtitle">
            <strong>Performance:</strong> In terms of contact state accuracy, our MI-HGNN achieves a mean accuracy of 87.7%, improving the performance of the second best ECNN model by <strong>11.3%</strong>. Compared with the morphology-agnostic CNN model, our model outperforms it by <strong>22.1%</strong>. 
         </p>
        </div>
        
        <div class="item">
          <img src="https://raw.githubusercontent.com/lunarlab-gatech/Morphology-Informed-HGNN/refs/heads/main/paper/website_images/figure3a.png" width="1000">
        </div>
      </div>

      <div class="container-side-by-side">
        <div class="text" style="margin-right: 75px;">
          <p class="subtitle">
            <strong>Generalization:</strong> The real-world and simulated test sets for our experiments incorporate a variety of unseen parameters, including terrains, frictions, and speeds, which demonstrate the generalization ability of the MI-HGNN on out-of-distribution data. 
         </p>
        </div>
        <div class="item">
          <img src="https://raw.githubusercontent.com/lunarlab-gatech/Morphology-Informed-HGNN/refs/heads/main/paper/website_images/Table3.png" width="950">
        </div>
      </div>

      <div class="container-side-by-side">
        <div class="text" style="margin-right: 75px;">
          <p class="subtitle">
            <strong>Model Efficiency:</strong> Using our smallest MI-HGNN model, we reduce the model parameters from ECNN by a factor of <strong>482</strong> and still improve performance by <strong>8.4%</strong>. Therefore, we can achieve performance gains while simultaneously preserving computing resources.
         </p>
        </div>
        <div class="item">
          <img src="https://raw.githubusercontent.com/lunarlab-gatech/Morphology-Informed-HGNN/refs/heads/main/paper/website_images/Table2.png" width="880">
        </div>
      </div>

      <div class="container-side-by-side">
        <div class="text" style="margin-right: 75px;">
          <p class="subtitle">
          <strong>Sample Efficiency:</strong> MI-HGNN performance does not drop significantly until the number of training samples is reduced to <strong>10%</strong> of the entire training set. In addition, competitive results can still be achieved by our model using only <strong>2.5%</strong> of the training set.
         </p>
        </div>
        <div class="item">
          <img src="https://raw.githubusercontent.com/lunarlab-gatech/Morphology-Informed-HGNN/refs/heads/main/paper/website_images/figure3b.png" width="960">
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item">
        Your image here
        <img src="https://raw.githubusercontent.com/lunarlab-gatech/Morphology-Informed-HGNN/refs/heads/main/paper/website_images/figure5.png" alt="Evaluation of all model types on a sub sequence of the “Unseen All” test sequence for the GRF estimation task." width="700" class="center"/>
        <h2 class="subtitle has-text-centered">
          Evaluation of all model types on a sub sequence of the “Unseen All” test sequence for the GRF estimation task, which includes an unseen friction coefficient ( ˙x = 0.5), unseen speed (v = 1.0), and unseen terrain (rough).
        </h2>
      </div>
    </div>
  </div>
</section> -->

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/4xxM36RNRXc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{butterfield2024mi,
  title={MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception},
  author={Butterfield, Daniel and Garimella, Sandilya Sai and Cheng, Nai-Jen and Gan, Lu},
  journal={arXiv preprint arXiv:2409.11146},
  year={2024},
  eprint={2409.11146},
  url={https://arxiv.org/abs/2409.11146},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            <br><br>
            This page was built upon the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, but must link back to these pages and the license.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
